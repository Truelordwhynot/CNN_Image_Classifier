{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aaf7f511-607f-4468-b022-b1fd71acecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23f92982-1fe8-47b7-a3e1-d8ada2bb02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class StyleGANConfig:\n",
    "    def __init__(self):\n",
    "        # Basic settings\n",
    "        self.image_size = 256\n",
    "        self.style_dim = 512\n",
    "        self.latent_dim = 512\n",
    "        self.n_mlp = 8\n",
    "        \n",
    "        # Training settings\n",
    "        self.batch_size = 8\n",
    "        self.lr = 0.0002\n",
    "        self.beta1 = 0.0\n",
    "        self.beta2 = 0.99\n",
    "        \n",
    "        # Device settings\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Mapping Network - Converts latent vectors to style vectors\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, style_dim, n_mlp):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # First layer from latent_dim to style_dim\n",
    "        layers.append(nn.Linear(latent_dim, style_dim))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        # Additional layers\n",
    "        for _ in range(n_mlp - 1):\n",
    "            layers.append(nn.Linear(style_dim, style_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Normalize input\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return self.mapping(z)\n",
    "\n",
    "# Style-based Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.style_dim = config.style_dim\n",
    "        \n",
    "        # Mapping network\n",
    "        self.mapping = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                EqualLinear(config.style_dim, config.style_dim),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ) for _ in range(config.n_mlp)]\n",
    "        )\n",
    "        \n",
    "        # Initial input\n",
    "        self.input = nn.Parameter(torch.randn(1, config.style_dim, 4, 4))\n",
    "        \n",
    "        # Progressive blocks with consistent dimensions\n",
    "        channels = [512, 512, 256, 128, 64, 32]\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_chan = config.style_dim\n",
    "        \n",
    "        for out_chan in channels:\n",
    "            self.blocks.append(StyleBlock(in_chan, out_chan, config.style_dim))\n",
    "            in_chan = out_chan\n",
    "            \n",
    "        # RGB output\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.Conv2d(channels[-1], 3, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Map to style space\n",
    "        w = z\n",
    "        for layer in self.mapping:\n",
    "            w = layer(w)\n",
    "        \n",
    "        # Start from learned constant\n",
    "        x = self.input.repeat(z.shape[0], 1, 1, 1)\n",
    "        \n",
    "        # Apply style blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, w)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        return self.to_rgb(x)\n",
    "\n",
    "# Style Block with AdaIN\n",
    "class StyleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, style_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = ModulatedConv2d(in_channels, out_channels, 3, style_dim)\n",
    "        self.noise1 = NoiseInjection()\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x = self.conv1(x, w)\n",
    "        x = self.noise1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "        \n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, style_dim=512):\n",
    "        super().__init__()\n",
    "        self.norm = nn.InstanceNorm2d(channels, affine=False)\n",
    "        self.style = EqualLinear(style_dim, channels * 2)  # Using EqualLinear for better stability\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        style = self.style(w).unsqueeze(2).unsqueeze(3)\n",
    "        gamma, beta = style.chunk(2, 1)\n",
    "        return (1 + gamma) * self.norm(x) + beta\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "        self.scale = (1 / math.sqrt(in_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = F.linear(input, self.weight * self.scale, self.bias)\n",
    "        return out\n",
    "        \n",
    "# Simple Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Progressive downsample\n",
    "        self.main = nn.Sequential(\n",
    "            # 256x256 -> 128x128\n",
    "            nn.Conv2d(3, 16, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 128x128 -> 64x64\n",
    "            nn.Conv2d(16, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 64x64 -> 32x32\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Final classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Dataset loader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "# Set multiprocessing start method\n",
    "try:\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "class CelebAHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=256):\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.image_paths = list(self.root_dir.glob(\"*.jpg\"))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.image_paths[idx])  # Convert Path to string\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            return self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank image in case of error\n",
    "            return torch.zeros(3, self.transform.transforms[0].size, \n",
    "                             self.transform.transforms[0].size)\n",
    "\n",
    "def create_dataloader(root_dir, config):\n",
    "    \"\"\"Create dataloader with error handling\"\"\"\n",
    "    try:\n",
    "        dataset = CelebAHQDataset(root_dir, config.image_size)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Set to 0 for debugging\n",
    "            pin_memory=True if torch.backends.mps.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {e}\")\n",
    "        return None\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None:\n",
    "            batch, _, height, width = x.shape\n",
    "            noise = torch.randn(batch, 1, height, width, device=x.device)\n",
    "        return x + self.weight * noise\n",
    "\n",
    "    \n",
    "# Training helper\n",
    "def train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, config):\n",
    "    batch_size = real_imgs.size(0)\n",
    "    device = config.device\n",
    "    \n",
    "    # Train Discriminator\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    # Real images\n",
    "    real_pred = discriminator(real_imgs)\n",
    "    d_real_loss = F.softplus(-real_pred).mean()\n",
    "    \n",
    "    # Fake images\n",
    "    z = torch.randn(batch_size, config.latent_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = generator(z)\n",
    "    fake_pred = discriminator(fake_imgs)\n",
    "    d_fake_loss = F.softplus(fake_pred).mean()\n",
    "    \n",
    "    d_loss = d_real_loss + d_fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # Train Generator\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    z = torch.randn(batch_size, config.latent_dim, device=device)\n",
    "    fake_imgs = generator(z)\n",
    "    fake_pred = discriminator(fake_imgs)\n",
    "    \n",
    "    g_loss = F.softplus(-fake_pred).mean()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    # Clear cache for M1\n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'd_loss': d_loss.item(),\n",
    "        'g_loss': g_loss.item(),\n",
    "        'd_real': torch.sigmoid(real_pred).mean().item(),\n",
    "        'd_fake': torch.sigmoid(fake_pred).mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7a45e04-6fcb-4e89-9e59-3d41ea756039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "        self.scale = (1 / math.sqrt(in_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = F.linear(input, self.weight * self.scale, self.bias)\n",
    "        return out\n",
    "\n",
    "def save_sample_images(generator, config, name, num_samples=16, nrow=4):\n",
    "    \"\"\"Generate and save sample images\"\"\"\n",
    "    try:\n",
    "        os.makedirs('samples', exist_ok=True)\n",
    "        \n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, config.latent_dim, device=config.device)\n",
    "            fake_images = generator(z)\n",
    "            fake_images = (fake_images + 1) / 2\n",
    "            \n",
    "            save_path = f'samples/generated_{name}.png'\n",
    "            vutils.save_image(fake_images.cpu(), save_path, nrow=nrow, normalize=False)\n",
    "            print(f'\\nSaved generated images at {save_path}')\n",
    "            \n",
    "        generator.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sample images: {e}\")\n",
    "\n",
    "def train_model(generator, discriminator, dataloader, config, num_epochs=10):\n",
    "    try:\n",
    "        # Optimizers\n",
    "        g_optimizer = torch.optim.Adam(\n",
    "            generator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        d_optimizer = torch.optim.Adam(\n",
    "            discriminator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        os.makedirs('samples', exist_ok=True)\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            epoch_metrics = []\n",
    "            \n",
    "            for batch_idx, real_imgs in enumerate(pbar):\n",
    "                try:\n",
    "                    real_imgs = real_imgs.to(config.device)\n",
    "                    metrics = train_step(\n",
    "                        real_imgs, generator, discriminator,\n",
    "                        g_optimizer, d_optimizer, config\n",
    "                    )\n",
    "                    epoch_metrics.append(metrics)\n",
    "                    \n",
    "                    # Update progress bar with rolling average\n",
    "                    if len(epoch_metrics) > 50:\n",
    "                        avg_metrics = {\n",
    "                            k: sum(m[k] for m in epoch_metrics[-50:]) / 50 \n",
    "                            for k in metrics.keys()\n",
    "                        }\n",
    "                        pbar.set_postfix(avg_metrics)\n",
    "                    \n",
    "                    # Save samples and checkpoint periodically\n",
    "                    if batch_idx % 500 == 0:\n",
    "                        try:\n",
    "                            save_sample_images(\n",
    "                                generator, \n",
    "                                config, \n",
    "                                f\"epoch_{epoch+1}_batch_{batch_idx}\"\n",
    "                            )\n",
    "                            \n",
    "                            torch.save({\n",
    "                                'epoch': epoch,\n",
    "                                'batch': batch_idx,\n",
    "                                'generator_state_dict': generator.state_dict(),\n",
    "                                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                                'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                                'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                            }, f'checkpoints/checkpoint_e{epoch}_b{batch_idx}.pt')\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error saving checkpoint: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"\\nError in batch {batch_idx}: {str(e)}\")\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        if torch.backends.mps.is_available():\n",
    "                            torch.mps.empty_cache()\n",
    "                        continue\n",
    "                    raise e\n",
    "            \n",
    "            # Save final state for epoch\n",
    "            try:\n",
    "                save_sample_images(generator, config, f\"epoch_{epoch+1}_final\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'generator_state_dict': generator.state_dict(),\n",
    "                    'discriminator_state_dict': discriminator.state_dict(),\n",
    "                    'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                    'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                }, f'checkpoints/checkpoint_epoch_{epoch+1}.pt')\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving final epoch state: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50a145e0-2cdc-4f48-81c6-4116c8303e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CelebAHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=256):\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        if not self.root_dir.exists():\n",
    "            raise ValueError(f\"Dataset directory {root_dir} does not exist!\")\n",
    "\n",
    "        # Look for images recursively in all subdirectories\n",
    "        self.image_paths = []\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "        \n",
    "        print(f\"Scanning {root_dir} for images...\")\n",
    "        for ext in valid_extensions:\n",
    "            self.image_paths.extend(list(self.root_dir.rglob(f\"*{ext}\")))\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(f\"No valid images found in {root_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        # Validate images\n",
    "        self.validate_images()\n",
    "        \n",
    "    def validate_images(self):\n",
    "        \"\"\"Validate images and remove corrupted ones\"\"\"\n",
    "        valid_paths = []\n",
    "        print(\"Validating images...\")\n",
    "        for img_path in tqdm(self.image_paths):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "                valid_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Corrupted image found {img_path}: {e}\")\n",
    "        \n",
    "        self.image_paths = valid_paths\n",
    "        print(f\"Found {len(self.image_paths)} valid images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.image_paths[idx])\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                return self.transform(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return torch.zeros(3, self.transform.transforms[0].size, \n",
    "                             self.transform.transforms[0].size)\n",
    "            \n",
    "def calc_path_lengths(fake_imgs, generator):\n",
    "    \"\"\"Calculate path lengths for path length regularization\"\"\"\n",
    "    batch_size = fake_imgs.shape[0]\n",
    "    noise = torch.randn_like(fake_imgs) / math.sqrt(fake_imgs.shape[2] * fake_imgs.shape[3])\n",
    "    \n",
    "    grad = torch.autograd.grad(\n",
    "        outputs=(fake_imgs * noise).sum(),\n",
    "        inputs=generator.parameters(),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n",
    "    return path_lengths\n",
    "    \n",
    "def find_dataset_path():\n",
    "    \"\"\"Find the CelebA-HQ dataset directory\"\"\"\n",
    "    possible_paths = [\n",
    "        'Data/celeba_hq',\n",
    "        './data/celeba_hq',\n",
    "        './datasets/celeba_hq',\n",
    "        '../celeba_hq',\n",
    "        '../data/celeba_hq',\n",
    "        '../datasets/celeba_hq',\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_dataloader(config):\n",
    "    \"\"\"Create dataloader with proper path finding and error handling\"\"\"\n",
    "    try:\n",
    "        # Find dataset path\n",
    "        dataset_path = find_dataset_path()\n",
    "        if dataset_path is None:\n",
    "            raise ValueError(\"Could not find CelebA-HQ dataset directory. Please specify the correct path.\")\n",
    "            \n",
    "        print(f\"Using dataset path: {dataset_path}\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = CelebAHQDataset(dataset_path, config.image_size)\n",
    "        \n",
    "        if len(dataset) == 0:\n",
    "            raise ValueError(\"Dataset is empty!\")\n",
    "            \n",
    "        # Create dataloader\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Set to 0 for debugging\n",
    "            pin_memory=True if torch.backends.mps.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Helper function to calculate log2\n",
    "def log2(x):\n",
    "    return int(torch.log2(torch.tensor(x)).item())\n",
    "    \n",
    "def compute_gradient_penalty(discriminator, real_imgs, fake_imgs, device):\n",
    "    \"\"\"Compute gradient penalty for improved WGAN training\"\"\"\n",
    "    batch_size = real_imgs.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_imgs + ((1 - alpha) * fake_imgs)).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, style_dim):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.style_dim = style_dim\n",
    "        \n",
    "        self.scale = 1 / math.sqrt(in_channels * kernel_size ** 2)\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.modulation = EqualLinear(style_dim, in_channels)\n",
    "        self.demodulate = True\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        batch, in_channels, height, width = x.shape\n",
    "        \n",
    "        # Style modulation\n",
    "        style = self.modulation(style).view(batch, 1, -1, 1, 1)\n",
    "        weight = self.scale * self.weight * style\n",
    "        \n",
    "        if self.demodulate:\n",
    "            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
    "            weight = weight * demod.view(batch, self.out_channels, 1, 1, 1)\n",
    "        \n",
    "        weight = weight.view(\n",
    "            batch * self.out_channels, in_channels, self.kernel_size, self.kernel_size\n",
    "        )\n",
    "        \n",
    "        x = x.view(1, batch * in_channels, height, width)\n",
    "        out = F.conv2d(x, weight, padding=self.padding, groups=batch)\n",
    "        out = out.view(batch, self.out_channels, height, width)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set seeds for reproducibility\n",
    "    set_seeds()\n",
    "    \n",
    "    # Create config\n",
    "    config = StyleGANConfig()\n",
    "    print(f\"Using device: {config.device}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator(config).to(config.device)\n",
    "    discriminator = Discriminator(config).to(config.device)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = create_dataloader(config)\n",
    "    if dataloader is None:\n",
    "        return\n",
    "        \n",
    "    # Train model\n",
    "    success = train_model(generator, discriminator, dataloader, config)\n",
    "    \n",
    "    if success:\n",
    "        print(\"Training completed successfully\")\n",
    "        # Save final samples\n",
    "        save_sample_images(generator, config, \"final\", num_samples=25, nrow=5)\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "        }, 'final_model.pt')\n",
    "    else:\n",
    "        print(\"Training failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5f70dea-d677-4c9b-bf25-bb43f9b50c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using dataset path: Data/celeba_hq\n",
      "Scanning Data/celeba_hq for images...\n",
      "Found 28000 images\n",
      "Validating images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 28000/28000 [00:03<00:00, 7769.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28000 valid images\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                      | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved generated images at samples/generated_epoch_1_batch_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  14%|▏| 500/3500 [04:43<28:07,  1.78it/s, d_loss=0.000242, g_loss=8."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved generated images at samples/generated_epoch_1_batch_500.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  29%|▎| 1000/3500 [09:32<23:29,  1.77it/s, d_loss=0.0762, g_loss=11."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved generated images at samples/generated_epoch_1_batch_1000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  43%|▍| 1500/3500 [14:19<19:21,  1.72it/s, d_loss=0.106, g_loss=10.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved generated images at samples/generated_epoch_1_batch_1500.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  43%|▍| 1516/3500 [14:28<18:56,  1.75it/s, d_loss=0.112, g_loss=12.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 216\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[75], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(generator, discriminator, dataloader, config, num_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     real_imgs \u001b[38;5;241m=\u001b[39m real_imgs\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 57\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     epoch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Update progress bar with rolling average\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 268\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(real_imgs, generator, discriminator, g_optimizer, d_optimizer, config)\u001b[0m\n\u001b[1;32m    266\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, config\u001b[38;5;241m.\u001b[39mlatent_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 268\u001b[0m     fake_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m fake_pred \u001b[38;5;241m=\u001b[39m discriminator(fake_imgs)\n\u001b[1;32m    270\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(fake_pred)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/Documents/AI_ML/CNN_Image_Classifier/venv_cnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AI_ML/CNN_Image_Classifier/venv_cnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[74], line 93\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Apply style blocks\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Convert to RGB\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_rgb(x)\n",
      "File \u001b[0;32m~/Documents/AI_ML/CNN_Image_Classifier/venv_cnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AI_ML/CNN_Image_Classifier/venv_cnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[74], line 107\u001b[0m, in \u001b[0;36mStyleBlock.forward\u001b[0;34m(self, x, w)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, w):\n\u001b[0;32m--> 107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, w)\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise1(x)\n",
      "File \u001b[0;32m~/Documents/AI_ML/CNN_Image_Classifier/venv_cnn/lib/python3.12/site-packages/torch/nn/functional.py:4580\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4571\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   4572\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_xpu\n\u001b[1;32m   4573\u001b[0m         ):\n\u001b[1;32m   4574\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4575\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4576\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4577\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m   4578\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4579\u001b[0m             )\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24105f0b-09fe-410d-b594-f1643b4cc358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1054e7bd-ca4a-4ef8-9d33-aa23314a82ad",
   "metadata": {},
   "source": [
    "# Actual Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82d178af-88e6-4702-884f-3363eadcc509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl ... done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "# Download StyleGAN2-ADA FFHQ pretrained weights\n",
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "with dnnlib.util.open_url(url) as f:\n",
    "    pretrained = legacy.load_network_pkl(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8fe5c8c9-eee6-49f8-8385-4b5e2e821f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "class PretrainedStyleGAN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.style_dim = config.style_dim\n",
    "        \n",
    "        # Mapping network\n",
    "        self.mapping = MappingNetwork(\n",
    "            latent_dim=config.style_dim,\n",
    "            style_dim=config.style_dim,\n",
    "            n_mlp=config.n_mlp\n",
    "        )\n",
    "        \n",
    "        # Synthesis network\n",
    "        self.synthesis = SynthesisNetwork(\n",
    "            style_dim=config.style_dim,\n",
    "            channels=config.channels,\n",
    "            max_resolution=config.image_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Map latent to style space\n",
    "        w = self.mapping(z)\n",
    "        # Generate image\n",
    "        return self.synthesis(w)\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, lr_mul=1.0):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.lr_mul = lr_mul\n",
    "        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            out = F.linear(input, self.weight * self.scale)\n",
    "        else:\n",
    "            out = F.linear(input, self.weight * self.scale, self.bias * self.lr_mul)\n",
    "        return out\n",
    "\n",
    "class SynthesisNetwork(nn.Module):\n",
    "    def __init__(self, style_dim, channels, max_resolution):\n",
    "        super().__init__()\n",
    "        self.style_dim = style_dim\n",
    "        self.max_resolution = max_resolution\n",
    "        self.log_size = int(math.log2(max_resolution))\n",
    "        \n",
    "        # Initial learned constant input\n",
    "        self.input = nn.Parameter(torch.randn(1, channels[4], 4, 4))\n",
    "        \n",
    "        # Style blocks (renamed from self.blocks)\n",
    "        self.style_blocks = nn.ModuleList()\n",
    "        in_channel = channels[4]\n",
    "        \n",
    "        # Build progressive blocks\n",
    "        for i in range(3, self.log_size + 1):\n",
    "            res = 2 ** i\n",
    "            out_channel = channels[res]\n",
    "            \n",
    "            self.style_blocks.append(\n",
    "                StyleBlock(\n",
    "                    in_channel,\n",
    "                    out_channel,\n",
    "                    style_dim,\n",
    "                    upsample=True\n",
    "                )\n",
    "            )\n",
    "            in_channel = out_channel\n",
    "            \n",
    "    def forward(self, w):\n",
    "        # Initial constant input\n",
    "        x = self.input.repeat(w.size(0), 1, 1, 1)\n",
    "        \n",
    "        # Apply style blocks\n",
    "        for block in self.style_blocks:  # Using style_blocks instead of blocks\n",
    "            x = block(x, w)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "class StyleBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, style_dim, upsample=True):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        # First convolution\n",
    "        self.conv1 = ModulatedConv2d(\n",
    "            in_channel, \n",
    "            out_channel, \n",
    "            3, \n",
    "            style_dim\n",
    "        )\n",
    "        self.noise1 = NoiseInjection()\n",
    "        self.activate1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        # Second convolution\n",
    "        self.conv2 = ModulatedConv2d(\n",
    "            out_channel, \n",
    "            out_channel, \n",
    "            3, \n",
    "            style_dim\n",
    "        )\n",
    "        self.noise2 = NoiseInjection()\n",
    "        self.activate2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        # First conv block\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x = self.conv1(x, style)\n",
    "        x = self.noise1(x)\n",
    "        x = self.activate1(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.conv2(x, style)\n",
    "        x = self.noise2(x)\n",
    "        x = self.activate2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, style_dim, n_mlp):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dim = latent_dim\n",
    "        for _ in range(n_mlp):\n",
    "            layers.append(EqualLinear(dim, style_dim, lr_mul=0.01))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            dim = style_dim\n",
    "            \n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Normalize latent vector\n",
    "        z = F.normalize(z, dim=1)\n",
    "        # Map to W space\n",
    "        return self.mapping(z)\n",
    "\n",
    "# Training helper functions\n",
    "def train_with_pretrained(generator, discriminator, dataloader, config):\n",
    "    \"\"\"Training loop incorporating pretrained weights\"\"\"\n",
    "    \n",
    "    # Load pretrained state\n",
    "    state_dict = torch.load('pretrained_stylegan2_ffhq.pt', weights_only=False)\n",
    "    generator.load_state_dict(state_dict['g'], strict=False)\n",
    "    discriminator.load_state_dict(state_dict['d'], strict=False)\n",
    "    \n",
    "    # Freeze certain layers\n",
    "    for name, param in generator.named_parameters():\n",
    "        if 'mapping' in name or 'style' in name:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    # Modified training loop\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        for batch_idx, real_imgs in enumerate(dataloader):\n",
    "            # Train discriminator\n",
    "            d_loss = train_d(real_imgs, generator, discriminator, config)\n",
    "            \n",
    "            # Train generator (with style mixing)\n",
    "            g_loss = train_g_mixed(generator, discriminator, config)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch}/{config.num_epochs}] Batch [{batch_idx}] '\n",
    "                      f'd_loss: {d_loss:.4f} g_loss: {g_loss:.4f}')\n",
    "                \n",
    "            # Save samples periodically\n",
    "            if batch_idx % 500 == 0:\n",
    "                save_samples(generator, f'samples/epoch_{epoch}_batch_{batch_idx}.png')\n",
    "                \n",
    "    return g_losses, d_losses\n",
    "\n",
    "def train_g_mixed(generator, discriminator, config):\n",
    "    \"\"\"Generator training with style mixing regularization\"\"\"\n",
    "    batch_size = config.batch_size\n",
    "    mixing_prob = 0.9\n",
    "    \n",
    "    # Generate two sets of latents\n",
    "    z1 = torch.randn(batch_size, config.style_dim).to(config.device)\n",
    "    z2 = torch.randn(batch_size, config.style_dim).to(config.device)\n",
    "    \n",
    "    # Mix styles\n",
    "    if random.random() < mixing_prob:\n",
    "        crossover_point = random.randint(1, generator.num_layers - 1)\n",
    "        w1 = generator.mapping(z1)\n",
    "        w2 = generator.mapping(z2)\n",
    "        w = torch.cat([w1[:,:crossover_point], w2[:,crossover_point:]], dim=1)\n",
    "    else:\n",
    "        w = generator.mapping(z1)\n",
    "    \n",
    "    # Generate and optimize\n",
    "    fake_imgs = generator.synthesis(w)\n",
    "    fake_pred = discriminator(fake_imgs)\n",
    "    g_loss = F.softplus(-fake_pred).mean()\n",
    "    \n",
    "    return g_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d29ee9c0-61cd-491b-ab35-13e6ab914c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, style_dim):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.style_dim = style_dim\n",
    "        \n",
    "        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2)\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\n",
    "        )\n",
    "        \n",
    "        # Style modulation\n",
    "        self.modulation = EqualLinear(style_dim, in_channel)\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        batch, in_channel, height, width = x.shape\n",
    "        \n",
    "        # Style modulation\n",
    "        style = self.modulation(style).view(batch, 1, in_channel, 1, 1)\n",
    "        weight = self.scale * self.weight * style\n",
    "        \n",
    "        # Demodulation\n",
    "        demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
    "        weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\n",
    "        \n",
    "        # Reshape for grouped convolution\n",
    "        weight = weight.view(\n",
    "            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
    "        )\n",
    "        x = x.view(1, batch * in_channel, height, width)\n",
    "        \n",
    "        # Convolution\n",
    "        out = F.conv2d(x, weight, padding=self.padding, groups=batch)\n",
    "        \n",
    "        return out.view(batch, self.out_channel, height, width)\n",
    "\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, image, noise=None):\n",
    "        if noise is None:\n",
    "            batch, _, height, width = image.shape\n",
    "            noise = image.new_empty(batch, 1, height, width).normal_()\n",
    "        return image + self.weight * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0abe50da-cb68-4329-b731-9039a580e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StyleGANWeightLoader:\n",
    "    def __init__(self):\n",
    "        self.weights_dir = 'pretrained_weights'\n",
    "        os.makedirs(self.weights_dir, exist_ok=True)\n",
    "        \n",
    "    def download_weights(self):\n",
    "        \"\"\"Download official StyleGAN2 weights\"\"\"\n",
    "        # Official NVIDIA StyleGAN2-ADA FFHQ weights\n",
    "        url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl'\n",
    "        filename = os.path.join(self.weights_dir, 'stylegan2-ffhq.pkl')\n",
    "        \n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Downloading weights from {url}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(filename, 'wb') as f, tqdm(\n",
    "                desc=\"Downloading\",\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = f.write(data)\n",
    "                    pbar.update(size)\n",
    "                    \n",
    "        return filename\n",
    "\n",
    "    def load_weights(self):\n",
    "        \"\"\"Load weights safely\"\"\"\n",
    "        try:\n",
    "            weight_file = self.download_weights()\n",
    "            \n",
    "            print(\"Loading and converting weights...\")\n",
    "            with open(weight_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # Convert weights to compatible format\n",
    "            state_dict = self.convert_weights(data)\n",
    "            \n",
    "            # Save converted weights\n",
    "            torch.save(state_dict, os.path.join(self.weights_dir, 'converted_weights.pt'))\n",
    "            \n",
    "            return state_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weights: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def convert_weights(self, data):\n",
    "        \"\"\"Convert original weights to our format\"\"\"\n",
    "        g_state_dict = {}\n",
    "        d_state_dict = {}\n",
    "        \n",
    "        # Handle different weight formats\n",
    "        if isinstance(data, dict):\n",
    "            g_weights = data.get('G_ema', data.get('G'))\n",
    "            d_weights = data.get('D')\n",
    "        else:\n",
    "            g_weights = data\n",
    "            d_weights = None\n",
    "            \n",
    "        # Convert generator weights\n",
    "        if hasattr(g_weights, 'state_dict'):\n",
    "            g_state = g_weights.state_dict()\n",
    "        else:\n",
    "            g_state = g_weights\n",
    "            \n",
    "        # Map weights to our architecture\n",
    "        for name, param in g_state.items():\n",
    "            if isinstance(param, torch.Tensor):\n",
    "                if 'mapping' in name:\n",
    "                    new_name = name.replace('mapping.', 'mapping.layers.')\n",
    "                    g_state_dict[new_name] = param.clone()\n",
    "                elif 'synthesis' in name:\n",
    "                    if 'conv' in name:\n",
    "                        g_state_dict[name] = param.clone()\n",
    "                    elif 'noise' in name:\n",
    "                        g_state_dict[name] = param.clone()\n",
    "                    elif 'toRGB' in name:\n",
    "                        g_state_dict[name] = param.clone()\n",
    "                        \n",
    "        return {\n",
    "            'g': g_state_dict,\n",
    "            'd': d_state_dict if d_weights is not None else None\n",
    "        }\n",
    "\n",
    "        \n",
    "def load_pretrained_model(generator, discriminator, config):\n",
    "    \"\"\"Load pretrained weights into models\"\"\"\n",
    "    try:\n",
    "        loader = StyleGANWeightLoader()\n",
    "        weights = loader.load_weights()\n",
    "        \n",
    "        # Load weights with error handling\n",
    "        missing_g, unexpected_g = generator.load_state_dict(weights['g'], strict=False)\n",
    "        print(f\"\\nGenerator loading info:\")\n",
    "        print(f\"Missing keys: {len(missing_g)}\")\n",
    "        print(f\"Unexpected keys: {len(unexpected_g)}\")\n",
    "        \n",
    "        if weights['d']:\n",
    "            missing_d, unexpected_d = discriminator.load_state_dict(weights['d'], strict=False)\n",
    "            print(f\"\\nDiscriminator loading info:\")\n",
    "            print(f\"Missing keys: {len(missing_d)}\")\n",
    "            print(f\"Unexpected keys: {len(unexpected_d)}\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading pretrained weights: {str(e)}\")\n",
    "        print(\"Using random initialization instead.\")\n",
    "        return False\n",
    "\n",
    "def save_samples(fake_imgs, path, nrow=4):\n",
    "    \"\"\"Save generated samples with proper format conversion\"\"\"\n",
    "    try:\n",
    "        # Ensure the images are in the correct format\n",
    "        if fake_imgs.size(1) != 3:\n",
    "            fake_imgs = fake_imgs.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Convert from [-1, 1] to [0, 1]\n",
    "        fake_imgs = (fake_imgs + 1) / 2\n",
    "        fake_imgs = torch.clamp(fake_imgs, 0, 1)\n",
    "        \n",
    "        # Save the images\n",
    "        torchvision.utils.save_image(\n",
    "            fake_imgs,\n",
    "            path,\n",
    "            nrow=nrow,\n",
    "            normalize=False,\n",
    "            range=(0, 1)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving samples: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "def save_checkpoint(generator, discriminator, g_optim, d_optim, epoch, filename):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optim.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optim.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, os.path.join('checkpoints', filename))\n",
    "\n",
    "def train_stylegan(generator, discriminator, dataloader, config, num_epochs=10):\n",
    "    \"\"\"Main training loop with improved progress tracking\"\"\"\n",
    "    \n",
    "    # Setup optimizers\n",
    "    g_optim = torch.optim.Adam(\n",
    "        generator.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(config.beta1, config.beta2)\n",
    "    )\n",
    "    d_optim = torch.optim.Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(config.beta1, config.beta2)\n",
    "    )\n",
    "    \n",
    "    # Create directories for saving results\n",
    "    os.makedirs('samples', exist_ok=True)\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_metrics = []\n",
    "        \n",
    "        for i, real_imgs in enumerate(progress_bar):\n",
    "            try:\n",
    "                # Training step\n",
    "                metrics = train_step(real_imgs, generator, discriminator, g_optim, d_optim, config)\n",
    "                \n",
    "                if metrics is not None:\n",
    "                    epoch_metrics.append(metrics)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    avg_metrics = {\n",
    "                        k: sum(m[k] for m in epoch_metrics[-100:]) / len(epoch_metrics[-100:])\n",
    "                        for k in metrics.keys()\n",
    "                    }\n",
    "                    progress_bar.set_postfix(avg_metrics)\n",
    "                \n",
    "                # Save samples and checkpoint\n",
    "                if i % 500 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # Generate samples\n",
    "                        sample_z = torch.randn(16, config.style_dim).to(config.device)\n",
    "                        samples = generator(sample_z)\n",
    "                        torchvision.utils.save_image(\n",
    "                            samples,\n",
    "                            f'samples/epoch_{epoch}_batch_{i}.png',\n",
    "                            normalize=True,\n",
    "                            value_range=(-1, 1),\n",
    "                            nrow=4\n",
    "                        )\n",
    "                        \n",
    "                        # Save checkpoint\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'batch': i,\n",
    "                            'generator_state_dict': generator.state_dict(),\n",
    "                            'discriminator_state_dict': discriminator.state_dict(),\n",
    "                            'g_optimizer_state_dict': g_optim.state_dict(),\n",
    "                            'd_optimizer_state_dict': d_optim.state_dict(),\n",
    "                            'metrics': avg_metrics\n",
    "                        }, f'checkpoints/checkpoint_e{epoch}_b{i}.pt')\n",
    "                \n",
    "                # Memory management\n",
    "                if i % 100 == 0 and torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in batch {i}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # Save epoch summary\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'metrics': avg_metrics\n",
    "        }, f'checkpoints/epoch_{epoch}_summary.pt')\n",
    "        \n",
    "    return epoch_metrics\n",
    "\n",
    "def load_and_train(generator, discriminator, dataloader, config):\n",
    "    \"\"\"Load weights and start training\"\"\"\n",
    "    \n",
    "    # Initialize weight loader\n",
    "    weight_loader = StyleGANWeightLoader()\n",
    "    \n",
    "    try:\n",
    "        # Load pretrained weights\n",
    "        print(\"Loading pretrained StyleGAN2 weights...\")\n",
    "        weights = weight_loader.load_stylegan2_weights()\n",
    "        \n",
    "        # Load weights into models\n",
    "        generator.load_state_dict(weights['g'], strict=False)\n",
    "        if weights['d'] is not None:\n",
    "            discriminator.load_state_dict(weights['d'], strict=False)\n",
    "            \n",
    "        print(\"Successfully loaded pretrained weights\")\n",
    "        \n",
    "        # Start training\n",
    "        return train_with_pretrained(generator, discriminator, dataloader, config)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights: {str(e)}\")\n",
    "        print(\"Continuing with randomly initialized weights...\")\n",
    "        return train_with_pretrained(generator, discriminator, dataloader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b0680de6-b0f7-4f9c-8a54-c3dab21852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_initialize(generator, discriminator, config):\n",
    "    \"\"\"Load weights and initialize models\"\"\"\n",
    "    loader = StyleGANWeightLoader()\n",
    "    weights = loader.load_weights()\n",
    "    \n",
    "    if weights is not None:\n",
    "        print(\"Loading pretrained weights...\")\n",
    "        missing_g, unexpected_g = generator.load_state_dict(weights['g'], strict=False)\n",
    "        print(f\"\\nGenerator loading stats:\")\n",
    "        print(f\"Missing keys: {len(missing_g)}\")\n",
    "        print(f\"Unexpected keys: {len(unexpected_g)}\")\n",
    "        \n",
    "        if weights['d'] is not None:\n",
    "            missing_d, unexpected_d = discriminator.load_state_dict(weights['d'], strict=False)\n",
    "            print(f\"\\nDiscriminator loading stats:\")\n",
    "            print(f\"Missing keys: {len(missing_d)}\")\n",
    "            print(f\"Unexpected keys: {len(unexpected_d)}\")\n",
    "    else:\n",
    "        print(\"Using random initialization\")\n",
    "\n",
    "\n",
    "def create_dataloader(config):\n",
    "    \"\"\"Create dataloader with proper error handling\"\"\"\n",
    "    try:\n",
    "        # Verify dataset path exists\n",
    "        if not os.path.exists(config.data_dir):\n",
    "            raise ValueError(f\"Dataset directory {config.data_dir} not found!\")\n",
    "            \n",
    "        # Create dataset\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(config.image_size),\n",
    "            transforms.CenterCrop(config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        class CelebADataset(Dataset):\n",
    "            def __init__(self, root_dir, transform=None):\n",
    "                self.root_dir = Path(root_dir)\n",
    "                self.transform = transform\n",
    "                self.image_paths = list(self.root_dir.glob('*.jpg'))\n",
    "                \n",
    "                if len(self.image_paths) == 0:\n",
    "                    raise ValueError(f\"No images found in {root_dir}\")\n",
    "                    \n",
    "                print(f\"Found {len(self.image_paths)} images\")\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.image_paths)\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                img_path = self.image_paths[idx]\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image\n",
    "                \n",
    "        dataset = CelebADataset(config.data_dir, transform=transform)\n",
    "        \n",
    "        # Create dataloader with proper batch size\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=True if torch.backends.mps.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        return dataloader\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "class StyleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, style_dim, upsample=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = ModulatedConv2d(\n",
    "            in_channels, out_channels, 3, style_dim, upsample=upsample\n",
    "        )\n",
    "        self.noise1 = NoiseInjection()\n",
    "        self.activation1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv2 = ModulatedConv2d(\n",
    "            out_channels, out_channels, 3, style_dim\n",
    "        )\n",
    "        self.noise2 = NoiseInjection()\n",
    "        self.activation2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        x = self.conv1(x, style)\n",
    "        x = self.noise1(x)\n",
    "        x = self.activation1(x)\n",
    "        \n",
    "        x = self.conv2(x, style)\n",
    "        x = self.noise2(x)\n",
    "        x = self.activation2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def save_image_grid(images, path, nrow=4):\n",
    "    \"\"\"Save a grid of images\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torchvision.utils.save_image(\n",
    "        images,\n",
    "        path,\n",
    "        nrow=nrow,\n",
    "        normalize=True,\n",
    "        range=(-1, 1)\n",
    "    )\n",
    "\n",
    "class StyleGANConfig:\n",
    "    def __init__(self):\n",
    "        # Model architecture\n",
    "        self.image_size = 256\n",
    "        self.style_dim = 512\n",
    "        self.n_mlp = 8\n",
    "        self.channels = {\n",
    "            4: 512,    # 4x4\n",
    "            8: 512,    # 8x8\n",
    "            16: 512,   # 16x16\n",
    "            32: 512,   # 32x32\n",
    "            64: 256,   # 64x64\n",
    "            128: 128,  # 128x128\n",
    "            256: 64    # 256x256\n",
    "        }\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 16\n",
    "        self.lr = 0.002\n",
    "        self.beta1 = 0.0\n",
    "        self.beta2 = 0.99\n",
    "        \n",
    "        # Dataset parameters\n",
    "        self.data_dir = 'Data/celeba_hq'\n",
    "        self.num_workers = 0\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "05ac2b15-6856-46c9-b713-827215aee637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGANTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = PretrainedStyleGAN(config).to(self.device)\n",
    "        self.discriminator = Discriminator(config).to(self.device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.g_optim = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        self.d_optim = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs('samples', exist_ok=True)\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        \n",
    "    def train(self, dataloader, num_epochs=10):\n",
    "        print(f\"Starting training on {self.device}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            epoch_metrics = []\n",
    "            \n",
    "            for i, real_imgs in enumerate(progress_bar):\n",
    "                try:\n",
    "                    metrics = train_step(\n",
    "                        real_imgs, \n",
    "                        self.generator,\n",
    "                        self.discriminator,\n",
    "                        self.g_optim,\n",
    "                        self.d_optim,\n",
    "                        self.config,\n",
    "                        self.device\n",
    "                    )\n",
    "                    \n",
    "                    if metrics is not None:\n",
    "                        epoch_metrics.append(metrics)\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        if len(epoch_metrics) > 0:\n",
    "                            avg_metrics = {\n",
    "                                k: sum(m[k] for m in epoch_metrics[-100:] if k != 'fake_imgs') / min(len(epoch_metrics), 100)\n",
    "                                for k in metrics.keys() if k != 'fake_imgs'\n",
    "                            }\n",
    "                            progress_bar.set_postfix(avg_metrics)\n",
    "                        \n",
    "                        # Save samples\n",
    "                        if i % 500 == 0:\n",
    "                            save_samples(\n",
    "                                metrics['fake_imgs'],\n",
    "                                f'samples/epoch_{epoch}_batch_{i}.png'\n",
    "                            )\n",
    "                            \n",
    "                            # Save checkpoint\n",
    "                            self.save_checkpoint(epoch, i, metrics)\n",
    "                            \n",
    "                    # Memory management\n",
    "                    if i % 100 == 0 and torch.backends.mps.is_available():\n",
    "                        torch.mps.empty_cache()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in batch {i}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "                    \n",
    "            # Save epoch summary\n",
    "            self.save_epoch_summary(epoch, epoch_metrics)\n",
    "            \n",
    "    def save_checkpoint(self, epoch, batch, metrics):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'batch': batch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': self.g_optim.state_dict(),\n",
    "            'd_optimizer_state_dict': self.d_optim.state_dict(),\n",
    "            'metrics': {k: v for k, v in metrics.items() if k != 'fake_imgs'}\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoints/checkpoint_e{epoch}_b{batch}.pt')\n",
    "        \n",
    "    def save_epoch_summary(self, epoch, metrics):\n",
    "        summary = {\n",
    "            'epoch': epoch,\n",
    "            'metrics': {\n",
    "                k: sum(m[k] for m in metrics if k != 'fake_imgs') / len(metrics)\n",
    "                for k in metrics[0].keys() if k != 'fake_imgs'\n",
    "            }\n",
    "        }\n",
    "        torch.save(summary, f'checkpoints/epoch_{epoch}_summary.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31877232-ec90-4963-811b-852c2c1034a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e54e9dab-10cf-465e-bb9b-feb19e1e21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "class StyleGANTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = Generator(config).to(self.device)\n",
    "        self.discriminator = Discriminator(config).to(self.device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.g_optim = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        self.d_optim = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        # Initialize scaler for mixed precision\n",
    "        self.scaler = torch.amp.GradScaler(enabled=False)  # Disabled for MPS\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs('samples', exist_ok=True)\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        \n",
    "        # Set up M1 optimizations\n",
    "        if self.device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "            \n",
    "    def train_step(self, real_imgs):\n",
    "        \"\"\"Single training step for both G and D\"\"\"\n",
    "        batch_size = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(self.device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        self.d_optim.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Real images\n",
    "        real_pred = self.discriminator(real_imgs)\n",
    "        d_real_loss = F.softplus(-real_pred).mean()\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, self.config.style_dim).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            fake_imgs = self.generator(z)\n",
    "        fake_pred = self.discriminator(fake_imgs.detach())\n",
    "        d_fake_loss = F.softplus(fake_pred).mean()\n",
    "        \n",
    "        # Combined D loss\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        self.d_optim.step()\n",
    "\n",
    "        # Train Generator\n",
    "        self.g_optim.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Generate new fake images\n",
    "        z = torch.randn(batch_size, self.config.style_dim).to(self.device)\n",
    "        fake_imgs = self.generator(z)\n",
    "        fake_pred = self.discriminator(fake_imgs)\n",
    "        \n",
    "        # G loss\n",
    "        g_loss = F.softplus(-fake_pred).mean()\n",
    "        g_loss.backward()\n",
    "        self.g_optim.step()\n",
    "\n",
    "        # Memory management\n",
    "        if self.device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        return {\n",
    "            'd_loss': d_loss.item(),\n",
    "            'g_loss': g_loss.item(),\n",
    "            'fake_images': fake_imgs.detach()\n",
    "        }\n",
    "\n",
    "    def save_samples(self, fake_imgs, epoch, batch_idx):\n",
    "        \"\"\"Save generated samples\"\"\"\n",
    "        try:\n",
    "            samples_dir = Path('samples')\n",
    "            samples_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Convert to range [0, 1]\n",
    "            fake_imgs = (fake_imgs + 1) / 2.0\n",
    "            fake_imgs = torch.clamp(fake_imgs, 0, 1)\n",
    "            \n",
    "            # Create image grid\n",
    "            grid = torchvision.utils.make_grid(\n",
    "                fake_imgs[:16],  # Save top 16 images\n",
    "                nrow=4,\n",
    "                padding=2,\n",
    "                normalize=False\n",
    "            )\n",
    "            \n",
    "            # Save grid\n",
    "            save_path = samples_dir / f'samples_epoch_{epoch}_batch_{batch_idx}.png'\n",
    "            torchvision.utils.save_image(grid, save_path)\n",
    "            print(f\"\\nSaved samples to {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving samples: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def save_checkpoint(self, epoch, batch_idx, losses):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'batch_idx': batch_idx,\n",
    "                'generator_state': self.generator.state_dict(),\n",
    "                'discriminator_state': self.discriminator.state_dict(),\n",
    "                'g_optimizer': self.g_optim.state_dict(),\n",
    "                'd_optimizer': self.d_optim.state_dict(),\n",
    "                'losses': losses,\n",
    "                'config': self.config\n",
    "            }\n",
    "            \n",
    "            save_path = f'checkpoints/styleGAN_epoch{epoch}_batch{batch_idx}.pt'\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f\"Saved checkpoint: {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    def train(self, dataloader, num_epochs=10):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting training on {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(num_epochs):\n",
    "                pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "                running_d_loss = 0\n",
    "                running_g_loss = 0\n",
    "                \n",
    "                for batch_idx, real_imgs in enumerate(pbar):\n",
    "                    # Training step\n",
    "                    metrics = self.train_step(real_imgs)\n",
    "                    \n",
    "                    # Update running losses\n",
    "                    running_d_loss = 0.9 * running_d_loss + 0.1 * metrics['d_loss']\n",
    "                    running_g_loss = 0.9 * running_g_loss + 0.1 * metrics['g_loss']\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'D_loss': f\"{running_d_loss:.4f}\",\n",
    "                        'G_loss': f\"{running_g_loss:.4f}\"\n",
    "                    })\n",
    "                    \n",
    "                    # Save samples and checkpoint periodically\n",
    "                    if batch_idx % 100 == 0:\n",
    "                        self.save_samples(metrics['fake_images'], epoch, batch_idx)\n",
    "                        self.save_checkpoint(\n",
    "                            epoch, \n",
    "                            batch_idx,\n",
    "                            {\n",
    "                                'g_loss': running_g_loss,\n",
    "                                'd_loss': running_d_loss\n",
    "                            }\n",
    "                        )\n",
    "                        \n",
    "                    # Memory management\n",
    "                    if batch_idx % 10 == 0 and self.device.type == \"mps\":\n",
    "                        torch.mps.empty_cache()\n",
    "                \n",
    "                # Save epoch checkpoint\n",
    "                self.save_checkpoint(\n",
    "                    epoch,\n",
    "                    len(dataloader),\n",
    "                    {\n",
    "                        'g_loss': running_g_loss,\n",
    "                        'd_loss': running_d_loss\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nTraining interrupted: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    \"\"\"Made pickleable for multiprocessing\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_paths = []\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "        \n",
    "        for ext in valid_extensions:\n",
    "            self.image_paths.extend(\n",
    "                [str(p) for p in Path(root_dir).glob(f'*{ext}')]\n",
    "            )\n",
    "            \n",
    "        if not self.image_paths:\n",
    "            raise RuntimeError(f\"No images found in {root_dir}\")\n",
    "            \n",
    "        print(f\"Found {len(self.image_paths)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return torch.zeros(3, self.transform.transforms[0].size,\n",
    "                             self.transform.transforms[0].size)\n",
    "\n",
    "def create_dataloader(config):\n",
    "    \"\"\"Create dataloader with proper multiprocessing settings\"\"\"\n",
    "    try:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(config.image_size),\n",
    "            transforms.CenterCrop(config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        dataset = CelebADataset(config.data_dir, transform)\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Set back to 0 for debugging\n",
    "            pin_memory=True if torch.backends.mps.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1e891138-fd47-4d21-ad48-155a4f1d2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.style_dim = config.style_dim\n",
    "        self.num_layers = int(math.log2(config.image_size)) - 1\n",
    "        self.gradient_checkpointing_enabled = False\n",
    "        \n",
    "        # Mapping Network\n",
    "        mapping_layers = []\n",
    "        for _ in range(config.n_mlp):\n",
    "            mapping_layers.extend([\n",
    "                EqualLinear(config.style_dim, config.style_dim),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ])\n",
    "        self.mapping = nn.Sequential(*mapping_layers)\n",
    "        \n",
    "        # Initial learned constant input\n",
    "        self.input = nn.Parameter(torch.randn(1, config.channels[4], 4, 4))\n",
    "        \n",
    "        # Style blocks for progressive generation\n",
    "        self.style_blocks = nn.ModuleList()\n",
    "        in_channel = config.channels[4]\n",
    "        \n",
    "        resolutions = [4, 8, 16, 32, 64, 128, 256]  # All possible resolutions\n",
    "        for i in range(len(resolutions) - 1):\n",
    "            curr_res = resolutions[i]\n",
    "            next_res = resolutions[i + 1]\n",
    "            \n",
    "            self.style_blocks.append(\n",
    "                StyleBlock(\n",
    "                    in_channel, \n",
    "                    config.channels[next_res],\n",
    "                    config.style_dim,\n",
    "                    upsample=True\n",
    "                )\n",
    "            )\n",
    "            in_channel = config.channels[next_res]\n",
    "            \n",
    "        # Final RGB conversion\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 3, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def enable_gradient_checkpointing(self):\n",
    "        self.gradient_checkpointing_enabled = True\n",
    "        \n",
    "    def disable_gradient_checkpointing(self):\n",
    "        self.gradient_checkpointing_enabled = False\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, z, return_latents=False):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        # Map to W space\n",
    "        if self.gradient_checkpointing_enabled and self.training:\n",
    "            w = torch.utils.checkpoint.checkpoint(self.mapping, z)\n",
    "        else:\n",
    "            w = self.mapping(z)\n",
    "        \n",
    "        # Replicate w for each style block if needed\n",
    "        if w.ndim == 2:\n",
    "            w = w.unsqueeze(1).repeat(1, len(self.style_blocks), 1)\n",
    "            \n",
    "        # Start from learned constant\n",
    "        x = self.input.repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "        # Apply style blocks with optional checkpointing\n",
    "        for i, block in enumerate(self.style_blocks):\n",
    "            if self.gradient_checkpointing_enabled and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(block, x, w[:, i])\n",
    "            else:\n",
    "                x = block(x, w[:, i])\n",
    "            \n",
    "        # Convert to RGB\n",
    "        out = self.to_rgb(x)\n",
    "        \n",
    "        if return_latents:\n",
    "            return out, w\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial RGB processing\n",
    "        self.from_rgb = nn.Sequential(\n",
    "            nn.Conv2d(3, config.channels[256], 1),  # Start with channels for highest resolution\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Progressive downsampling blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        resolutions = [256, 128, 64, 32, 16, 8, 4]  # From highest to lowest\n",
    "        \n",
    "        for i in range(len(resolutions) - 1):\n",
    "            curr_res = resolutions[i]\n",
    "            next_res = resolutions[i + 1]\n",
    "            \n",
    "            self.blocks.append(\n",
    "                DiscriminatorBlock(\n",
    "                    config.channels[curr_res],\n",
    "                    config.channels[next_res],\n",
    "                    downsample=True\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Final convolution layers\n",
    "        final_channels = config.channels[4]\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(final_channels, final_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(final_channels, final_channels, 4, padding=0),  # Changed to remove padding\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size\n",
    "        self.final_size = final_channels  # This will be 512 based on your config\n",
    "        \n",
    "        # Final classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.final_size, 1),  # Changed from final_channels * 4 * 4\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Initial RGB processing\n",
    "        x = self.from_rgb(x)  # x shape: [batch_size, 64, 256, 256]\n",
    "        \n",
    "        # Progressive downsampling\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # Progressive downsampling through resolutions\n",
    "            \n",
    "        # Final convolutions to 1x1\n",
    "        x = self.final_conv(x)  # Should result in [batch_size, 512, 1, 1]\n",
    "        \n",
    "        # Flatten properly\n",
    "        x = x.view(x.size(0), -1)  # Reshape to [batch_size, 512]\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, in_channel, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        if downsample:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, 4, stride=2, padding=1),  # Changed kernel and stride\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, 3, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, style_dim):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.style_dim = style_dim\n",
    "        \n",
    "        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2)\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\n",
    "        )\n",
    "        \n",
    "        self.modulation = EqualLinear(style_dim, in_channel)\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        batch, in_channel, height, width = x.shape\n",
    "        \n",
    "        # Style modulation\n",
    "        style = self.modulation(style).view(batch, 1, -1, 1, 1)\n",
    "        weight = self.scale * self.weight * style\n",
    "        \n",
    "        # Demodulation\n",
    "        demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
    "        weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\n",
    "        \n",
    "        # Reshape for grouped convolution\n",
    "        weight = weight.view(\n",
    "            batch * self.out_channel, in_channel, \n",
    "            self.kernel_size, self.kernel_size\n",
    "        )\n",
    "        x = x.view(1, batch * in_channel, height, width)\n",
    "        \n",
    "        # Convolution\n",
    "        out = F.conv2d(x, weight, padding=self.padding, groups=batch)\n",
    "        \n",
    "        return out.view(batch, self.out_channel, height, width)\n",
    "\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, image, noise=None):\n",
    "        if noise is None:\n",
    "            batch, _, height, width = image.shape\n",
    "            noise = image.new_empty(batch, 1, height, width).normal_()\n",
    "        return image + self.weight * noise\n",
    "\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, lr_mul=1.0):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.lr_mul = lr_mul\n",
    "        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            out = F.linear(input, self.weight * self.scale)\n",
    "        else:\n",
    "            out = F.linear(input, self.weight * self.scale, \n",
    "                         self.bias * self.lr_mul)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f376b6e7-c557-45c5-a029-236336605aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGANConfig:\n",
    "    def __init__(self):\n",
    "        # Model architecture\n",
    "        self.image_size = 256\n",
    "        self.style_dim = 512\n",
    "        self.n_mlp = 8\n",
    "        self.channels = {\n",
    "            4: 512,    # 4x4\n",
    "            8: 512,    # 8x8 \n",
    "            16: 512,   # 16x16\n",
    "            32: 512,   # 32x32\n",
    "            64: 256,   # 64x64\n",
    "            128: 128,  # 128x128\n",
    "            256: 64    # 256x256\n",
    "        }\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 32  # Increased for faster training\n",
    "        self.lr = 0.0002\n",
    "        self.beta1 = 0.0\n",
    "        self.beta2 = 0.99\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.mixed_precision = True\n",
    "        self.gradient_checkpointing = True\n",
    "        self.accumulation_steps = 4  # Gradient accumulation steps\n",
    "        \n",
    "        # System settings\n",
    "        self.num_workers = 2\n",
    "        self.pin_memory = True\n",
    "        self.prefetch_factor = 2\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "af85b7ec-9ac3-42a3-a2d8-e49feb86d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = StyleGANConfig()\n",
    "generator = Generator(config).to(config.device)\n",
    "discriminator = Discriminator(config).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "735695d9-a2e6-4296-8ab1-4797e4a08faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28000 images\n",
      "Starting training on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|         | 0/875 [00:16<?, ?it/s, D_loss=0.1447, G_loss=0.0396]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved samples to samples/samples_epoch_0_batch_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%| | 1/875 [00:17<4:20:13, 17.86s/it, D_loss=0.1447, G_loss=0.039"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: checkpoints/styleGAN_epoch0_batch0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  11%| | 100/875 [22:09<2:50:16, 13.18s/it, D_loss=1.3863, G_loss=0.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved samples to samples/samples_epoch_0_batch_100.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  12%| | 101/875 [22:10<2:54:10, 13.50s/it, D_loss=1.3863, G_loss=0.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: checkpoints/styleGAN_epoch0_batch100.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  18%|▏| 160/875 [35:40<2:39:24, 13.38s/it, D_loss=1.3863, G_loss=0.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[186], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m         success \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[182], line 150\u001b[0m, in \u001b[0;36mStyleGANTrainer.train\u001b[0;34m(self, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m    146\u001b[0m running_g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, real_imgs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Update running losses\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     running_d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m running_d_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[182], line 70\u001b[0m, in \u001b[0;36mStyleGANTrainer.train_step\u001b[0;34m(self, real_imgs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Generate new fake images\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m fake_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(z)\n\u001b[1;32m     72\u001b[0m fake_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(fake_imgs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize config\n",
    "    config = StyleGANConfig()\n",
    "    config.batch_size = 32\n",
    "    config.lr = 0.0002\n",
    "    config.data_dir = \"Data/celeba_hq\"\n",
    "    \n",
    "    # Create trainer and dataloader\n",
    "    trainer = StyleGANTrainer(config)\n",
    "    dataloader = create_dataloader(config)\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        try:\n",
    "            success = trainer.train(dataloader, num_epochs=10)\n",
    "            if success:\n",
    "                print(\"Training completed successfully!\")\n",
    "            else:\n",
    "                print(\"Training completed with errors.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {str(e)}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ba3f7f6-75b3-47ca-95dc-caf4ef361bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "with dnnlib.util.open_url(url) as f:\n",
    "    pretrained = legacy.load_network_pkl(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f0deba-bbd5-4816-a4fd-117afc5e41c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
