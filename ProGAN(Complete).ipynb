{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad9a29b-eaad-4889-a102-6f9f389862af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7075afe0-a924-4dce-864d-be5112b3fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32] # Feature map for adjusting for each resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17182269-6337-431c-b39f-97471d64fa5c",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf07aab-c4ba-4235-9b67-a7320a1f43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module): # Weight Scaled 2D\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding) # Initializes the conv layer\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5 # Calculates the scaling factor\n",
    "        self.bias = self.conv.bias # Sets a manual Bias\n",
    "        self.conv.bias = None # Uses self.bias instead of conv.bias\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight) # Weights initialized (Normal Distribution)\n",
    "        nn.init.zeros_(self.bias) # Bias is a tensor of zeros\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1) # Input is scaled to prevent 'Vanishing Gradient'\n",
    "                                                                                       # Bias reshaped to match output dimesnion then applies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3de61c-4aed-487f-8b7c-cbba11d24109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8 # Prevents division by zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon) # Normalizing the vector across all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba1ad31-655b-4b9a-81e7-59380d5555aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):l\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels) # Allows stacking of Conv Blocks\n",
    "        self.leaky = nn.LeakyReLU(0.2) # Prevents vanishing gradient (Negative values have contribution)\n",
    "        self.pn = PixelNorm() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x # Computes the tensor and returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18f4696-93c5-468c-80a9-85b8edf8c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3): # Takes a Random Noise of z_dim\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.initial = nn.Sequential( # 1x1 latent vector -> 4x4\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0), # 1x1->4x4,\n",
    "            nn.LeakyReLU(0.2), # Prevents dying neurons\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        ) # Building board for the generator (Canvas)\n",
    "\n",
    "        self.initial_rgb = WSConv2d( # Converts 4x4 into a RGB Image\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0 \n",
    "        ) # Returns a low res image\n",
    "        self.prog_blocks, self.rgb_layers = ( # Allows stacking of layers (Resolutions)\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]), # Converts feature maps into RGB at each step\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1]) # Each block increases resolution \n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c)) # Learning is done here\n",
    "            self.rgb_layers.append( # Converts output into RGB\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled) # allows the images to Fade Into higher resolution smoothly\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out) # Prevents error for 1 channel dimension (apparently works)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out) # Smooth Fade in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc323f5-8d18-4866-9848-ce00261701c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) - 1, 0, -1): # Setup Blocks in reverse order (High Res To Low Res)\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False)) # Pixel Norm is not essential for generators\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d( # Downsamples image by factor of 2 (Intermediate between blocks)\n",
    "            kernel_size=2, stride=2\n",
    "        )  \n",
    "        \n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d( # Final Output Indicates real or fake\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out): # Fades in for lower res\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x): # Prevents generator mode collapse\n",
    "        batch_statistics = ( # Calcs std \n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3]) # Adds std as as cahnnel\n",
    "        )\n",
    "        return torch.cat([x, batch_statistics], dim=1) # Generator produces varies results\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        cur_step = len(self.prog_blocks) - steps # Start RES\n",
    "\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0: # Intial step adds the minibatch channel\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a121b2-c64a-4b1b-99b7-45d6307bc3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/yw087q412xl2x40kqhfyj6740000gn/T/ipykernel_39574/3122939803.py:38: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! At img size: 4\n",
      "Success! At img size: 8\n",
      "Success! At img size: 16\n",
      "Success! At img size: 32\n",
      "Success! At img size: 64\n",
      "Success! At img size: 128\n",
      "Success! At img size: 256\n",
      "Success! At img size: 512\n",
      "Success! At img size: 1024\n"
     ]
    }
   ],
   "source": [
    "# Test cases to check if all fuctions run for all cases. (Check Change_logs/Sources.txt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Z_DIM = 50\n",
    "    IN_CHANNELS = 256\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "    critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "\n",
    "    for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        num_steps = int(log2(img_size / 4))\n",
    "        x = torch.randn((1, Z_DIM, 1, 1))\n",
    "        z = gen(x, 0.5, steps=num_steps)\n",
    "        assert z.shape == (1, 3, img_size, img_size)\n",
    "        out = critic(z, alpha=0.5, steps=num_steps)\n",
    "        assert out.shape == (1, 1)\n",
    "        print(f\"Success! At img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e5b49-4c93-4461-85ea-e8eba384db39",
   "metadata": {},
   "source": [
    "# Model Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c922f949-9bee-489b-b908-5da8395f9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d010f9-a3f0-439a-80a0-c000f3e41880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step) # Log data onto tensorboard\n",
    "\n",
    "    with torch.no_grad(): # Create image grid\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step) # Log image grids\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ee04a5-ddc2-425b-8a9b-fe9186e1cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WGAN-GP in change_logs/source.txt\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"): # Critic: Score for generator\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta) # Images b/w real and fake\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step) #Computes gradient penalty\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1) # Calculates Gradient Nomr (Magnitude penalty)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2) # Enforces  Lipschitz continuity\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc4aab07-4296-48d8-a25a-7c904f02c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"): # Save checkpoints to resume later (Useful for less ram computations)\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3910c5-dac1-418a-a2ba-27f8244e33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008719fd-adc9-42b2-9bd0-f61c949f8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(gen, steps, truncation=0.7, n=100): # Demo purposes\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad(): # Truncation for noise control\n",
    "            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, config.Z_DIM, 1, 1)), device=config.DEVICE, dtype=torch.float32)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7a2b3-2e0b-4cb8-9273-c8ad4d1a0280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eba508e-9fc3-43e4-91a6-f5d549d27c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from math import log2\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Intial Parameters\n",
    "START_TRAIN_AT_IMG_SIZE = 128 # Intial Res\n",
    "DATASET = 'celeba_hq_org'\n",
    "CHECKPOINT_GEN = \"generator.pth\" # File saving location\n",
    "CHECKPOINT_CRITIC = \"critic.pth\"\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # Use cuda if Nvidia GPU Available\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZES = [16, 16, 16, 8, 8, 8, 8, 4, 2] # Change batch_size with respect to ram needs\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 256  \n",
    "IN_CHANNELS = 256 \n",
    "CRITIC_ITERATIONS = 1 # 5 for WGAN performance\n",
    "LAMBDA_GP = 10 # Gradient penalty weight\n",
    "PROGRESSIVE_EPOCHS = [10] * len(BATCH_SIZES) # No of epoches\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE) # Noise generation\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6faaaf1-cad7-4b59-9fd6-56e2a2548f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/yw087q412xl2x40kqhfyj6740000gn/T/ipykernel_39574/2056571070.py:1: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  batch_statistics = (torch.std(x, dim=0) + 1e-8).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n"
     ]
    }
   ],
   "source": [
    "batch_statistics = (torch.std(x, dim=0) + 1e-8).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c675e70-d033-4c78-b719-157e164d4be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a94d65-a3e1-440e-8636-2fcc91844ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmarks = True # Selects most eff algo\n",
    "\n",
    "\n",
    "def get_loader(image_size): # Fetch images\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)), # Resize as a square, consistent\n",
    "            transforms.ToTensor(), # Converts to tensor E(0,1)\n",
    "            transforms.RandomHorizontalFlip(p=0.5), \n",
    "            transforms.Normalize( # Normalise with mean = 0.5, std= 0.5 \n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3138685d-e837-4027-b627-f9e08c409a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    tensorboard_step,\n",
    "    writer,\n",
    "    scaler_gen,\n",
    "    scaler_critic,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n",
    "        # which is equivalent to minimizing the negative of the expression\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n",
    "\n",
    "        with torch.amp.autocast(device_type='mps'):\n",
    "            fake = gen(noise, alpha, step)\n",
    "            critic_real = critic(real, alpha, step)\n",
    "            critic_fake = critic(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        with torch.amp.autocast(device_type='mps'):\n",
    "            gen_fake = critic(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        # Update alpha and ensure less than 1\n",
    "        alpha += cur_batch_size / (\n",
    "            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69aea00-a3ef-4880-b0d1-2eec32464dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987beb94-0ed9-40f1-b552-aefb00732782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ecf3448-67c2-4ee7-91cc-9ce1f92911c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # initialize gen and disc, note: discriminator should be called critic,\n",
    "    # according to WGAN paper (since it no longer outputs between [0, 1])\n",
    "    # but really who cares..\n",
    "    gen = Generator(\n",
    "        Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
    "    ).to(DEVICE)\n",
    "    critic = Discriminator(\n",
    "        Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # initialize optimizers and scalers for FP16 training\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    opt_critic = optim.Adam(\n",
    "        critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n",
    "    )\n",
    "    scaler_critic = torch.amp.GradScaler()\n",
    "    scaler_gen = torch.amp.GradScaler()\n",
    "\n",
    "    # for tensorboard plotting\n",
    "    writer = SummaryWriter(f\"logs/gan1\")\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    # start at step that corresponds to img size that we set in config\n",
    "    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5  # start with very low alpha\n",
    "        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train_fn(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer,\n",
    "                scaler_gen,\n",
    "                scaler_critic,\n",
    "            )\n",
    "\n",
    "            if SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
    "\n",
    "        step += 1  # progress to the next img size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea071c63-db2b-487b-adf5-5658cc680718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivaram/Documents/AI_ML/CNN_Image_Classifier/venv_cnn/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image size: 128\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████▍    | 2331/3750 [47:46<28:54,  1.22s/it, gp=0.0392, loss_critic=-7]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7ce5b-12c1-4bf7-a554-7e93b2f4c195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
